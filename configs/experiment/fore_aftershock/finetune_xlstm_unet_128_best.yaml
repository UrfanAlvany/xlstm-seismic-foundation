# @package _global_

defaults:
  - /trainer: default
  - /loader: default
  - /dataset: foreshock_aftershock
  - /task: classification

experiment_name: XLSTM_UNET_finetune_foreshock_best128

model:
  _name_: xlstm-unet
  d_model: 128
  n_layers: 3
  pool: [4, 4]
  expand: 2
  dropout: 0.1
  block_type: mlstm
  bidirectional: true
  max_seq_len: 4096
  mlstm_num_heads: 4
  mlstm_qkv_proj_blocksize: 4
  mlstm_conv1d_kernel_size: 4
  mlstm_proj_factor: 2.0
  mlstm_backend: chunkwise
  ff_proj_factor: 1.3
  ff_act_fn: gelu
  enable_tflakernels: true
  chunk_size: 128          # aligns with 2048 pad multiple
  chunkwise_kernel: chunkwise--triton_xl_chunk
  sequence_kernel: native_sequence__triton
  step_kernel: triton
  autocast_kernel_dtype: bfloat16
  gradient_checkpointing: false
  pretrained: /scicore/home/dokman0000/alvani0000/final_seismology/wandb_logs/mars/contrastive_resume_58609395.ckpt/checkpoints/epoch=39-step=605261.ckpt

encoder:
  pretrained: true
  freeze: true

decoder:
  _name_: sequence-classifier
  mode: double-conv
  kernel_size: 3
  dropout: 0.2
  num_classes: 9
  manual_input_dim: 128

dataset:
  _name_: foreshock-aftershock
  data_dir: /scicore/home/dokman0000/alvani0000/seis_data
  num_classes: 9
  batch_size: 32
  event_split_method: temporal
  component_order: ZNE
  seed: 42
  train_frac: 0.7
  val_frac: 0.1
  test_frac: 0.2
  dimension_order: NWC
  demean_axis: 1
  amp_norm_axis: 1
  amp_norm_type: std
  num_workers: 8

loader:
  batch_size: 64
  num_workers: 8
  pin_memory: true

optimizer:
  _name_: adamw
  lr: 4e-4
  weight_decay: 0.1
  betas: [0.9, 0.95]
  fused: true
  foreach: false

scheduler:
  _name_: cosine_warmup
  num_warmup_steps: 0
  num_training_steps: 0
  warmup_fraction: 0.0

task:
  _name_: classification
  loss: cross-entropy
  metrics: accuracy

train:
  monitor: val/loss
  mode: min
  interval: step
  disable_pretraining: true
  disable_mlstm_kernels: false

trainer:
  accelerator: gpu
  strategy: auto
  devices: 1
  accumulate_grad_batches: 1
  max_epochs: 15
  gradient_clip_val: 0.0
  log_every_n_steps: 10
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  enable_model_summary: false
  precision: bf16-mixed
  val_check_interval: 0.5


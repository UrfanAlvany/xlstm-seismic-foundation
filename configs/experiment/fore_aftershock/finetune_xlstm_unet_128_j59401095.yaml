# @package _global_

defaults:
  - /trainer: default
  - /loader: default
  - /dataset: foreshock_aftershock
  - /task: classification

experiment_name: XLSTM_UNET_finetune_foreshock

model:
  _name_: xlstm-unet
  d_model: 128
  n_layers: 3
  pool: [4, 4]
  expand: 2
  dropout: 0.1
  block_type: mlstm
  bidirectional: true
  max_seq_len: 4096
  # sLSTM (unused when block_type=mlstm but kept for parity)
  slstm_num_heads: 4
  slstm_conv1d_kernel_size: 4
  slstm_bias_init: powerlaw_blockdependent
  # mLSTM internals (match pretraining)
  mlstm_num_heads: 4
  mlstm_qkv_proj_blocksize: 4
  mlstm_conv1d_kernel_size: 4
  mlstm_proj_factor: 2.0
  mlstm_backend: chunkwise
  ff_proj_factor: 1.3
  ff_act_fn: gelu
  # Triton kernels
  enable_tflakernels: true
  chunk_size: 128
  chunkwise_kernel: chunkwise--triton_xl_chunk
  sequence_kernel: native_sequence__triton
  step_kernel: triton
  autocast_kernel_dtype: bfloat16
  gradient_checkpointing: false
  # Pretrained contrastive checkpoint (symlinked in Slurm runs; can be overridden on CLI)
  pretrained: null  # set via CLI: pretrained=/path/to/file

encoder:
  pretrained: null  # set via CLI: pretrained=/path/to/file
  freeze: true

decoder:
  _name_: sequence-classifier
  mode: double-conv
  kernel_size: 3
  dropout: 0.2
  num_classes: 9
  manual_input_dim: 128   # pure 128 head (matches j59401095)

dataset:
  _name_: foreshock-aftershock
  data_dir: ${oc.env:SEIS_DATA_DIR,null}  # or set directly here
  num_classes: 9
  batch_size: 32
  event_split_method: temporal
  component_order: ZNE
  seed: 42
  train_frac: 0.7
  val_frac: 0.1
  test_frac: 0.2
  # xLSTM encoder expects NWC (time, channels last)
  dimension_order: NWC
  # SeisLM-style normalization behavior
  demean_axis: 1
  amp_norm_axis: 1
  amp_norm_type: std
  num_workers: 0

loader:
  batch_size: 64    # unused for foreshock dataloader; kept for parity
  num_workers: 0
  pin_memory: true

optimizer:
  _name_: adamw
  lr: 4e-4
  weight_decay: 0.1
  betas: [0.9, 0.95]
  fused: true
  foreach: false

scheduler:
  _name_: cosine_warmup
  num_warmup_steps: 0
  num_training_steps: 0
  warmup_fraction: 0.0

task:
  _name_: classification
  loss: cross-entropy
  metrics: accuracy

train:
  monitor: val/loss
  mode: min
  interval: step
  disable_pretraining: true
  disable_mlstm_kernels: false

trainer:
  accelerator: gpu
  strategy: auto
  devices: 1
  accumulate_grad_batches: 1
  max_epochs: 15
  gradient_clip_val: 0.0
  log_every_n_steps: 10
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  enable_model_summary: false
  precision: bf16-mixed
  val_check_interval: 0.5


# @package _global_
# Fine-tune GEOFON 20% with OLD pretraining checkpoint (mLSTM_Sequential model)
# FIXED VERSION: Corrected encoder freeze and weight decay bugs
#
# FIXES:
# 1. encoder.freeze: removed (was true, preventing unfreezing)
# 2. weight_decay: 0.005 (was 0.05 - 10x too high!)
# 3. experiment_name: corrected to 20pct (was wrongly 100pct)
#
# Based on successful GEOFON 50% run (2025-10-12__04_31_34)

defaults:
  - /trainer: default
  - /loader: default
  - /dataset: seisbench_phase
  - /task: phase_pick
  - /optimizer: adamw_mamba
  - /scheduler: cosine_warmup

experiment_name: XLSTM_phasepick_GEOFON_20pct_optimized_v2

model:
  pretrained: null  # set via CLI: pretrained=/path/to/file

dataset:
  sample_len: 4096
  bits: 0
  d_data: 3
  dataset_name: GEOFON
  norm_type: std
  training_fraction: 0.2  # 20% of data

decoder:
  _name_: double-conv-phase-pick
  kernel_size: 3
  dropout: 0.2
  upsample: True

encoder:
  pretrained: null  # set via CLI: pretrained=/path/to/file
  # FIX: Removed "freeze: true" to allow proper unfreezing at epoch 5

loader:
  batch_size: 64
  num_workers: 8
  pin_memory: True
  drop_last: True
  persistent_workers: True
  prefetch_factor: 2

optim:
  clip_grad_norm: 1.0

optimizer:
  _name_: adamw
  lr: 0.0002  # Matched to successful GEOFON 50% run
  weight_decay: 0.005  # FIX: Was 0.05 (10x too high!)
  betas: [0.9, 0.95]
  fused: True
  foreach: False

scheduler:
  _name_: cosine_warmup
  num_warmup_steps: null
  num_training_steps: null
  warmup_fraction: 0.1

task:
  _name_: phase_pick
  loss: phase-pick

train:
  monitor: val/loss
  mode: min
  optimizer_param_grouping:
    bias_weight_decay: False
    normalization_weight_decay: False
  interval: step
  unfreeze_at_epoch: 5  # This will now work correctly!
  disable_pretraining: True

trainer:
  accelerator: gpu
  strategy: auto
  devices: 1
  accumulate_grad_batches: 4
  max_epochs: 40  # Reduced from 50 to match successful runs
  gradient_clip_val: 0.0
  log_every_n_steps: 5
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  enable_model_summary: False
  precision: bf16-mixed
  max_steps: -1
  val_check_interval: 0.5  # More frequent than 0.25

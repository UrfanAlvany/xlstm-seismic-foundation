# @package _global_
# CEED fine-tuning with contrastive pretrained checkpoint
# Encoder: conv-down-encoder-contrastive
# Using lr=0.001

defaults:
  - /trainer: default
  - /loader: default
  - /dataset: seisbench_phase
  - /task: phase_pick
  - /optimizer: adamw_mamba
  - /scheduler: cosine_warmup

experiment_name: XLSTM_fine_tune_CEED_contrastive

model:
  pretrained: null  # set via CLI: pretrained=/path/to/file

dataset:
  _name_: ethz-phase-pick
  sample_len: 4096
  bits: 0
  d_data: 3
  dataset_name: CEED
  norm_type: std
  training_fraction: 1.0

encoder:
  pretrained: null  # set via CLI: pretrained=/path/to/file

decoder:
  _name_: double-conv-phase-pick
  upsample: true
  kernel_size: 3
  dropout: 0.2
  manual_input_dim: 256
  upsample_by: 4
  output_len: 4096

loader:
  batch_size: 64
  num_workers: 12

task:
  _name_: phase_pick
  loss: phase-pick

train:
  monitor: val/loss
  mode: min
  interval: step
  disable_pretraining: true

# lr=0.001 as requested
optimizer:
  lr: 0.001
  weight_decay: 0.1
  betas: [0.9, 0.95]
  fused: true
  foreach: false

scheduler:
  _name_: cosine_warmup
  warmup_fraction: 0.1

trainer:
  accelerator: gpu
  devices: 1
  max_epochs: 40
  precision: bf16-mixed
  val_check_interval: 0.5
  log_every_n_steps: 5
  accumulate_grad_batches: 2

# @package _global_
# PRIORITY 1: Fix ETHZ 50% dip (0.9904 â†’ target 0.9920+)
# Current run 2025-10-13__08_12_55 used lr=0.001, got 0.9904
# This version: Slightly lower LR + more epochs for stability

defaults:
  - /trainer: default
  - /loader: default
  - /dataset: seisbench_phase
  - /task: phase_pick
  - /optimizer: adamw_mamba
  - /scheduler: cosine_warmup

experiment_name: XLSTM_phasepick_ETHZ_50pct_v2_stable

model:
  pretrained: /scicore/home/dokman0000/alvani0000/final_seismology/wandb_logs/mars/contrastive_resume_58609395.ckpt/checkpoints/callback-epoch=38-step=592102.ckpt

dataset:
  _name_: ethz-phase-pick
  sample_len: 4096
  bits: 0
  d_data: 3
  dataset_name: ETHZ
  norm_type: std
  training_fraction: 0.5  # 50% data

decoder:
  _name_: double-conv-phase-pick
  kernel_size: 3
  dropout: 0.2
  manual_input_dim: 256
  upsample: true
  upsample_by: 4
  output_len: 4096

encoder:
  pretrained: true

loader:
  batch_size: 64
  num_workers: 8
  pin_memory: true
  drop_last: true
  persistent_workers: true
  prefetch_factor: 2

optim:
  clip_grad_norm: 1.0

optimizer:
  _name_: adamw
  lr: 0.0007  # Lower than 0.001 for more stable convergence
  weight_decay: 0.1
  betas: [0.9, 0.95]
  fused: true
  foreach: false

scheduler:
  _name_: cosine_warmup
  num_warmup_steps: null
  num_training_steps: null
  warmup_fraction: 0.1  # 10% warmup

task:
  _name_: phase_pick
  loss: phase-pick

train:
  monitor: val/loss
  mode: min
  optimizer_param_grouping:
    bias_weight_decay: false
    normalization_weight_decay: false
  interval: step
  disable_pretraining: true

trainer:
  accelerator: gpu
  strategy: auto
  devices: 1
  accumulate_grad_batches: 2  # Effective batch size: 128
  max_epochs: 40  # More epochs than original 30
  gradient_clip_val: 0.0
  log_every_n_steps: 5
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  enable_model_summary: false
  precision: bf16-mixed
  max_steps: -1
  val_check_interval: 0.5  # Check validation twice per epoch

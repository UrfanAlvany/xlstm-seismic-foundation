# @package _global_
# Resume mLSTM STEAD training from epoch 24 for 10 MORE epochs (to epoch 34)
# val/loss was still decreasing at 0.0177, needs more training

defaults:
  - /trainer: default
  - /loader: default
  - /dataset: seisbench_phase
  - /task: phase_pick
  - /optimizer: adamw_mamba
  - /scheduler: cosine_warmup

experiment_name: XLSTM_phasepick_STEAD_resume_10more

model:
  _name_: xlstm-unet
  pretrained: null  # set via CLI: pretrained=/path/to/file
  d_model: 12
  n_layers: 3
  pool: [4, 4]
  expand: 2
  dropout: 0.05
  block_type: mlstm
  bidirectional: true
  max_seq_len: 4096
  mlstm_num_heads: 2
  mlstm_conv1d_kernel_size: 4
  mlstm_qkv_proj_blocksize: 2
  mlstm_proj_factor: 2.0
  mlstm_backend: chunkwise
  ff_proj_factor: 1.6
  ff_act_fn: gelu
  gradient_checkpointing: false
  chunkwise_kernel: chunkwise--triton_limit_chunk
  sequence_kernel: native_sequence__triton
  step_kernel: triton
  chunk_size: 128
  autocast_kernel_dtype: bfloat16
  fuse_per_block: true

train:
  monitor: val/loss
  mode: min
  interval: step

dataset:
  sample_len: 4096
  bits: 0
  d_data: 3
  dataset_name: STEAD
  norm_type: std
  training_fraction: 1.0

encoder:
  _name_: linear

decoder:
  _name_: double-conv-phase-pick
  kernel_size: 5
  dropout: 0.1
  upsample: false

loader:
  batch_size: 64
  num_workers: 8
  drop_last: True
  persistent_workers: True
  prefetch_factor: 2

# Same optimizer & schedule as original
optimizer:
  lr: 0.002
  weight_decay: 0.1
  fused: true
  foreach: false

scheduler:
  _name_: cosine_warmup
  warmup_fraction: 0.1
  num_training_steps: null
  num_warmup_steps: null

trainer:
  max_steps: -1
  val_check_interval: 0.25
  devices: 1
  strategy: auto
  accelerator: gpu
  max_epochs: 34  # Resume from 24, train 10 more to reach 34
  precision: bf16-mixed
  enable_model_summary: false
  log_every_n_steps: 5
  gradient_clip_val: 0.0
  accumulate_grad_batches: 4

optim:
  clip_grad_norm: 1.0

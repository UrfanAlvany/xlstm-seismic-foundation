{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåã Foreshock-Aftershock Classification with xLSTM-UNet\n",
    "\n",
    "This notebook evaluates a fine-tuned xLSTM-UNet model on the Norcia earthquake foreshock/aftershock classification task.\n",
    "\n",
    "**Task**: Classify seismic events into 9 temporal classes:\n",
    "- 4 foreshock classes (FEQ1-FEQ4)\n",
    "- 1 Visso event class\n",
    "- 4 aftershock classes (AEQ1-AEQ4)\n",
    "\n",
    "**Model**: xLSTM-UNet fine-tuned from contrastive pretraining\n",
    "\n",
    "**Methodology**: Matches SeisLM's approach (temporal splitting, frozen encoder, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Add project to path\n",
    "# sys.path.insert(0, '/path/to/this/repo')\n",
    "\n",
    "from dataloaders.foreshock_aftershock_lit import ForeshockAftershockLitDataset\n",
    "from simple_train import SimpleSeqModel\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CHECKPOINT PATH ===\n",
    "CKPT_PATH = '/path/to/your/checkpoint_or_data'\n",
    "\n",
    "# === DATASET CONFIG ===\n",
    "DATA_DIR = '/path/to/your/checkpoint_or_data'\n",
    "NUM_CLASSES = 9\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# === DISPLAY LABELS ===\n",
    "DISPLAY_LABELS = [\n",
    "    \"FEQ1\",  # Foreshock class 1 (earliest)\n",
    "    \"FEQ2\",  # Foreshock class 2\n",
    "    \"FEQ3\",  # Foreshock class 3\n",
    "    \"FEQ4\",  # Foreshock class 4 (latest before main)\n",
    "    \"Visso\", # Visso event\n",
    "    \"AEQ1\",  # Aftershock class 1 (earliest after main)\n",
    "    \"AEQ2\",  # Aftershock class 2\n",
    "    \"AEQ3\",  # Aftershock class 3\n",
    "    \"AEQ4\",  # Aftershock class 4 (latest)\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Checkpoint: {CKPT_PATH}\")\n",
    "print(f\"‚úÖ Data directory: {DATA_DIR}\")\n",
    "print(f\"‚úÖ Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"‚úÖ Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Load Dataset\n",
    "\n",
    "Using the same configuration as training:\n",
    "- **Temporal splitting**: Events split temporally (prevents data leakage)\n",
    "- **Event-level split**: Different earthquakes in train/val/test\n",
    "- **Normalization**: std-norm per channel (seisLM-style)\n",
    "- **Component order**: ZNE\n",
    "- **Dimension order**: NWC (Batch, Width, Channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with same config as training\n",
    "dataset = ForeshockAftershockLitDataset(\n",
    "    data_dir=DATA_DIR,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    event_split_method='temporal',  # Match seisLM\n",
    "    component_order='ZNE',          # Match seisLM\n",
    "    seed=42,\n",
    "    remove_class_overlapping_dates=False,\n",
    "    train_frac=0.7,\n",
    "    val_frac=0.10,\n",
    "    test_frac=0.20,\n",
    "    dimension_order='NWC',          # Match seisLM\n",
    "    demean_axis=1,                  # Match seisLM (per channel)\n",
    "    amp_norm_axis=1,                # Match seisLM (per channel)\n",
    "    amp_norm_type='std',            # Match seisLM\n",
    "    num_workers=0,\n",
    "    collator=None,\n",
    ")\n",
    "\n",
    "test_loader = dataset.test_loader\n",
    "print(f\"‚úÖ Test set loaded: {len(test_loader.dataset)} samples\")\n",
    "print(f\"‚úÖ Number of batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Load Model\n",
    "\n",
    "Load the fine-tuned xLSTM-UNet model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model config directly from checkpoint (like SeisLM!)\n",
    "# This is the key: SeisLM loads the config FROM the checkpoint, not from experiment files\n",
    "print(\"Loading checkpoint...\")\n",
    "state = torch.load(CKPT_PATH, map_location=device, weights_only=False)\n",
    "\n",
    "# Extract hyperparameters that were used during training\n",
    "cfg = state['hyper_parameters']\n",
    "print(f\"Checkpoint model d_model: {cfg.model.d_model}\")\n",
    "\n",
    "# Disable struct mode to modify config\n",
    "OmegaConf.set_struct(cfg, False)\n",
    "\n",
    "# Disable pretrained loading (we already have the trained weights)\n",
    "if 'pretrained' in cfg.model:\n",
    "    cfg.model.pretrained = None\n",
    "\n",
    "# Add full encoder config (from pretrained checkpoint)\n",
    "# The checkpoint only has encoder.pretrained=true, but we need the full config\n",
    "if 'encoder' not in cfg or '_name_' not in cfg.encoder:\n",
    "    print(\"Adding encoder config from pretrained checkpoint...\")\n",
    "    cfg.encoder = OmegaConf.create({\n",
    "        '_name_': 'conv-down-encoder-contrastive',\n",
    "        'kernel_size': 3,\n",
    "        'n_layers': 2,\n",
    "        'dim': 256,\n",
    "        'stride': 2,\n",
    "        'pretrained': False,  # Don't reload, we'll load from checkpoint\n",
    "    })\n",
    "\n",
    "OmegaConf.set_struct(cfg, True)\n",
    "\n",
    "# Instantiate model with the SAME config as training\n",
    "print(\"Instantiating model...\")\n",
    "model = SimpleSeqModel(cfg, d_data=3).to(device)\n",
    "\n",
    "# Load the fine-tuned weights\n",
    "print(\"Loading trained weights...\")\n",
    "model.load_state_dict(state['state_dict'], strict=False)\n",
    "\n",
    "# Ensure classification mode (not pretraining)\n",
    "try:\n",
    "    model.model.pretraining = False\n",
    "    model.encoder.pretraining = False\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "model.eval()\n",
    "print(\"‚úÖ Model loaded successfully\")\n",
    "print(f\"‚úÖ Model in eval mode: {not model.training}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Evaluate Model\n",
    "\n",
    "Run inference on the test set and collect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_targets = []\n",
    "total_loss = 0.0\n",
    "n_samples = 0\n",
    "\n",
    "print(\"Running inference...\")\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x, y) in enumerate(test_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, targets = model.forward((x, y), batch_idx)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        total_loss += loss.item() * targets.shape[0]\n",
    "        n_samples += targets.shape[0]\n",
    "        \n",
    "        # Get predictions\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.detach().cpu().numpy())\n",
    "        all_targets.append(targets.detach().cpu().numpy())\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"  Processed {batch_idx + 1}/{len(test_loader)} batches\")\n",
    "\n",
    "# Concatenate all predictions\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "# Compute metrics\n",
    "test_loss = total_loss / max(1, n_samples)\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"üìä EVALUATION RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Total Samples: {n_samples}\")\n",
    "print(f\"{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Confusion Matrix\n",
    "\n",
    "Visualize the confusion matrix to see per-class performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_targets, all_preds, labels=list(range(NUM_CLASSES)))\n",
    "\n",
    "# Convert to percentages (per-row normalization)\n",
    "cm_percentage = 100 * cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Round to integer percentages\n",
    "cm_display = np.rint(cm_percentage).astype(int)\n",
    "\n",
    "print(\"Confusion Matrix (Percentages):\")\n",
    "print(cm_display)\n",
    "print(f\"\\nDiagonal (Per-class accuracy): {cm_display.diagonal()}\")\n",
    "print(f\"Mean diagonal accuracy: {cm_display.diagonal().mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix (SeisLM style)\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm_display,\n",
    "    display_labels=DISPLAY_LABELS,\n",
    ")\n",
    "\n",
    "disp.plot(ax=ax, xticks_rotation=45, colorbar=False, cmap=\"Reds\")\n",
    "ax.set_title(\n",
    "    f\"Confusion Matrix (xLSTM-UNet) | Accuracy: {test_acc*100:.2f}%\",\n",
    "    fontsize=16,\n",
    "    fontweight='bold'\n",
    ")\n",
    "ax.set_xlabel('Predicted Class', fontsize=12)\n",
    "ax.set_ylabel('True Class', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "save_path = '/path/to/your/checkpoint_or_data'\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "fig.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n‚úÖ Confusion matrix saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Per-Class Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class accuracy\n",
    "per_class_acc = cm_display.diagonal()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PER-CLASS ACCURACY\")\n",
    "print(\"=\"*60)\n",
    "for i, (label, acc) in enumerate(zip(DISPLAY_LABELS, per_class_acc)):\n",
    "    bar = '‚ñà' * int(acc / 5)  # Visual bar\n",
    "    print(f\"{label:8s} | {acc:3d}% {bar}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"MEAN     | {per_class_acc.mean():.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count samples per class\n",
    "unique, counts = np.unique(all_targets, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET CLASS DISTRIBUTION\")\n",
    "print(\"=\"*60)\n",
    "for i, label in enumerate(DISPLAY_LABELS):\n",
    "    count = class_distribution.get(i, 0)\n",
    "    print(f\"{label:8s} | {count:4d} samples\")\n",
    "print(\"=\"*60)\n",
    "print(f\"TOTAL    | {n_samples:4d} samples\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Comparison with SeisLM Methodology\n",
    "\n",
    "### ‚úÖ Implementation Checklist\n",
    "\n",
    "Your xLSTM-UNet implementation follows SeisLM's methodology:\n",
    "\n",
    "| **Aspect** | **SeisLM** | **Your xLSTM-UNet** | **Match?** |\n",
    "|------------|------------|---------------------|------------|\n",
    "| Dataset | Norcia foreshock/aftershock | Norcia foreshock/aftershock | ‚úÖ |\n",
    "| Num Classes | 9 | 9 | ‚úÖ |\n",
    "| Split Method | Temporal | Temporal | ‚úÖ |\n",
    "| Component Order | ZNE | ZNE | ‚úÖ |\n",
    "| Dimension Order | NWC | NWC | ‚úÖ |\n",
    "| Normalization | std per channel | std per channel | ‚úÖ |\n",
    "| Train/Val/Test | 70/10/20 | 70/10/20 | ‚úÖ |\n",
    "| Frozen Encoder | Yes | Yes | ‚úÖ |\n",
    "| Head Type | DoubleConv | DoubleConv | ‚úÖ |\n",
    "| Optimizer | AdamW (4e-4, wd=0.1) | AdamW (4e-4, wd=0.1) | ‚úÖ |\n",
    "| Max Epochs | 15 | 15 | ‚úÖ |\n",
    "| Pretrain Method | Contrastive | Contrastive | ‚úÖ |\n",
    "\n",
    "**Conclusion**: Your implementation correctly follows SeisLM's foreshock fine-tuning methodology! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file\n",
    "results = {\n",
    "    'checkpoint': CKPT_PATH,\n",
    "    'test_loss': float(test_loss),\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'num_samples': int(n_samples),\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'per_class_accuracy': per_class_acc.tolist(),\n",
    "    'class_labels': DISPLAY_LABELS,\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'confusion_matrix_percentage': cm_display.tolist(),\n",
    "}\n",
    "\n",
    "import json\n",
    "results_path = '/path/to/your/checkpoint_or_data'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Done!\n",
    "\n",
    "**Evaluation Complete** ‚úÖ\n",
    "\n",
    "Your xLSTM-UNet model has been evaluated on the foreshock-aftershock classification task using the exact same methodology as SeisLM.\n",
    "\n",
    "### Next Steps:\n",
    "1. Compare your accuracy with SeisLM's baseline\n",
    "2. Try evaluating other checkpoints (e.g., epoch=14)\n",
    "3. Analyze which classes are harder to classify\n",
    "4. Try with different num_classes (2, 4, 8) for comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (xlstm_official_240)",
   "language": "python",
   "name": "xlstm_official_240"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

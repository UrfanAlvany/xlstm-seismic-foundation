# @package _global_
# Template for xLSTM fine-tuning on SeisLM-style phase picking tasks
#
# Usage:
#   1. Copy this file to: fine_tune_xlstm_<DATASET>_seislm.yaml
#   2. Change dataset.dataset_name to your target dataset
#   3. (Optional) Update experiment_name
#   4. If dataset not recognized, add to dataloaders/seisbench_auto_reg.py
#
# Verified compatible with pretrained checkpoint:
#   wandb_logs/mars/contrastive_resume_58609395.ckpt/checkpoints/callback-epoch=38-step=592102.ckpt

defaults:
  - /trainer: default
  - /loader: default
  - /dataset: seisbench_phase
  - /task: phase_pick
  - /optimizer: adamw_mamba
  - /scheduler: cosine_warmup

experiment_name: XLSTM_phasepick_TEMPLATE_SeisLMStyle

model:
  # Best pretraining checkpoint (by val/loss) from WAV2VEC contrastive learning
  # Override via CLI: model.pretrained=/path/to/checkpoint.ckpt
  pretrained: final_seismology/wandb_logs/mars/contrastive_resume_58609395.ckpt/checkpoints/callback-epoch=38-step=592102.ckpt

train:
  monitor: val/loss
  mode: min
  interval: step

  # Ensure we use task loss (not contrastive) during fine-tuning
  disable_pretraining: True

dataset:
  sample_len: 4096
  bits: 0              # No quantization for supervised fine-tuning
  d_data: 3            # 3-component seismic data
  dataset_name: TEMPLATE  # <<< CHANGE THIS to your dataset (STEAD, ETHZ, GEOFON, VCSEIS, CEED, etc.)
  norm_type: std
  training_fraction: 1.0  # Override via CLI for fraction sweeps: dataset.training_fraction=0.05

encoder:
  pretrained: True     # Load encoder weights from checkpoint

decoder:
  _name_: double-conv-phase-pick
  kernel_size: 3
  dropout: 0.2
  # CRITICAL: Must match final_projection_dim from pretrained model (256)
  manual_input_dim: 256
  # Restore per-frame resolution for picking (encoder downsamples by 4x)
  upsample: true
  upsample_by: 4
  output_len: 4096

loader:
  batch_size: 64
  num_workers: 8
  drop_last: True
  persistent_workers: True
  prefetch_factor: 2

# SeisLM-style optimizer & schedule (verified optimal settings)
optimizer:
  lr: 0.001            # DO NOT CHANGE - very sensitive!
  weight_decay: 0.1
  fused: true
  foreach: false

scheduler:
  _name_: cosine_warmup
  # Let code infer steps; specify warmup fraction like SeisLM
  warmup_fraction: 0.1  # 10% warmup (tested vs 0.2 - this works better)
  num_training_steps: null
  num_warmup_steps: null

trainer:
  max_steps: -1
  val_check_interval: 0.5  # Validate twice per epoch
  devices: 1
  strategy: auto
  accelerator: gpu
  max_epochs: 30
  precision: bf16-mixed
  enable_model_summary: false
  log_every_n_steps: 5
  gradient_clip_val: 0.0
  accumulate_grad_batches: 2  # Effective batch size = 64 * 2 = 128

optim:
  clip_grad_norm: 1.0

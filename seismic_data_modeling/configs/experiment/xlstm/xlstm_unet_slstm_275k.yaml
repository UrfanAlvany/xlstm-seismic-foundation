# @package _global_
# xLSTM sLSTM U-Net - Target ~275k parameters (matching professor's baseline approach)
defaults:
  - /trainer: default
  - /loader: default
  - /dataset: seisbench_phase
  - /task: phase_pick
  - /optimizer: adamw_mamba        # Same as professor's SSM models
  - /scheduler: linear             # Same as professor's SSM models  
  - /model: xlstm_unet
  - _self_

experiment_name: xlstm_unet_slstm_287k_baseline

# MATCH PROFESSOR'S ENCODER/DECODER EXACTLY
encoder:
  _name_: linear                   # Same as all professor's small models

decoder:
  _name_: double-conv-phase-pick   # Same as all professor's models  
  upsample: False
  kernel_size: 5
  dropout: 0.1

# xLSTM sLSTM U-NET MODEL CONFIGURATION (Following professor's exact structure)
model:
  d_model: 16                      # Optimal for target parameter count (16 % 4 = 0)
  n_layers: 2                      # Fewer layers for parameter control
  block_type: "slstm"              # sLSTM blocks (more similar to S4/Mamba)
  bidirectional: true              # Same as professor's bidirectional SSMs
  expand: 2                        # Expansion factor 2 (professor's standard)
  pool:                            # Pooling by factor 4 (professor's standard)
    - 4
    - 4
  dropout: 0.0                     
  max_seq_len: 4096
  gradient_checkpointing: false    # Disable for initial testing
  slstm_num_heads: 4               # 16 % 4 = 0 (compatible)

# MATCH PROFESSOR'S TRAINING EXACTLY
train:
  monitor: val/loss
  mode: min

dataset:
  sample_len: 4096
  bits: 0
  d_data: 3
  dataset_name: ETHZ
  norm_type: std

loader:
  batch_size: 256                  # Same as mamba/sashimi
  drop_last: True
  persistent_workers: True         # Same as professor

# MATCH PROFESSOR'S OPTIMIZATION
scheduler:
  num_warmup_steps: 880            # Same as professor's SSM models
  num_training_steps: 8800

trainer:
  max_steps: 8800                  # Same as professor
  max_epochs: 100
  log_every_n_steps: 20
  gradient_clip_val: 1.0           # Same as professor

# SOLUTION TO PARAMETER DISCREPANCY:
# The key issue was U-Net vs simple stack architecture!
# 
# Professor's baselines (U-Net architecture):
# - MAMBA: d_model=8, n_layers=3, expand=2, pool=[4,4] → ~260k params
# - HYDRA: d_model=9, n_layers=3, expand=2, pool=[4,4] → ~261k params  
# - S4D: d_model=16, n_layers=8, expand=2, pool=[4,4] → ~271k params
# 
# Our xLSTM sLSTM U-Net solution:
# - sLSTM: d_model=16, n_layers=3, expand=2, pool=[4,4] → 307,728 params (+14.0%)
# - Mathematical estimate shows +14.0% difference from 270k target (within tolerance)
# - Uses same U-Net structure as professor's baselines for fair comparison
# - sLSTM architecture more similar to S4/Mamba state-space models
# - d_model=16 required for compatibility with num_heads=4 (16 % 4 = 0)